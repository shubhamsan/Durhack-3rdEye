# ğŸ§¿ 3rd Eye â€“ Raspberry Pi Head-Tracking Assistive Camera

**DurHack 2025 â€“ Assistive Technology & Robotics**

3rd Eye is an assistive-vision prototype designed to give visually impaired users a remote "second camera" that automatically follows their head movement. The system streams live video from a Raspberry Pi to a PC while using real-time face landmark tracking (Mediapipe) to rotate a servo-mounted camera.

Originally, the team aimed to use eye-gaze tracking, but due to time constraints and model limitations, we pivoted to a more robust nose-landmark head-tracking approach.

---

## ğŸ¯ Motivation

Blind and visually-impaired people often struggle to explore their surroundings independently. **3rd Eye** acts as a remote controllable viewpoint:

- The Raspberry Pi camera acts as an **external vision device**
- Your **head movements** control where it looks
- The PC receives **live video + provides UI feedback**

This allows the user to scan and explore spaces that they cannot physically reach.

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Face Tracking** | Mediapipe Face Landmarker v2 |
| **Communication** | ZeroMQ (PUSH/PULL), ImageZMQ |
| **Camera** | Raspberry Pi Camera Module (Picamera2) |
| **Servo Motor** | AngularServo (GPIOZero) |
| **UI / Display** | OpenCV on PC |
| **Language** | Python 3 (both PC + Raspberry Pi) |

> **Note:** No Node.js, web, or browser components were used in this implementation.

---

## âš™ï¸ System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PC (Python)         â”‚
â”‚  â”€ Head Tracking (MP)    â”‚
â”‚  â”€ UI + Servo Angle Calc â”‚
â”‚  â”€ Receives Pi Camera    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ ZMQ (angles)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Raspberry Pi          â”‚
â”‚  â”€ Camera Streaming      â”‚
â”‚  â”€ Receives Servo Angles â”‚
â”‚  â”€ Controls Servo Motor  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The Pi continuously streams frames â†’ PC, while the PC continuously sends angles â†’ Pi.

---

## ğŸ“ Project Structure

```
DurhackX/
â”‚
â”œâ”€â”€ camera_servo_zmq.py                           # PC-side tracker + display
â”œâ”€â”€ rpi_control.py                                # Pi-side camera + servo control
â”œâ”€â”€ face_landmarker_v2_with_blendshapes.task      # Mediapipe model
â””â”€â”€ README.md
```

---

## ğŸ§  How the Tracking Works

### 1. Real-Time Face Landmark Detection
Using **Mediapipe Tasks API**:
- Landmark #1 (nose tip) is extracted
- Nose horizontal position = head direction

### 2. Mapping Head Movement â†’ Servo Angle
```python
offset = nose_x - calibrated_center
angle_change = (offset / (screen_width / 4)) * sensitivity
servo_angle = clamp(90 + angle_change)
```
This gives smooth left-right pan control from **0Â° â†’ 180Â°**.

### 3. Sending the Angle to Raspberry Pi
- PC â†’ ZMQ PUSH â†’ Pi (port 5556)

### 4. Streaming Camera Feed Back to PC
- Pi â†’ ImageZMQ â†’ PC (port 5555)
- The received frame is shown in the corner of the PC UI.

---

## ğŸ› ï¸ Requirements & Installation

### ğŸ–¥ï¸ PC Setup

**Install dependencies:**
```bash
pip install opencv-python mediapipe numpy pyzmq imagezmq
```

**Place the Mediapipe model:**
- `face_landmarker_v2_with_blendshapes.task`

---

### ğŸ“ Raspberry Pi Setup

**Enable camera:**
```bash
sudo raspi-config
```

**Install dependencies:**
```bash
sudo apt update
sudo apt install python3-picamera2
pip install gpiozero pyzmq imagezmq numpy
```

---

## â–¶ï¸ How to Run

### ğŸš€ 1. Start Raspberry Pi Controller

**Edit PC IP in `rpi_control.py`:**
```python
PC_IP = "your_pc_ip_here"
```

**Run:**
```bash
python3 rpi_control.py
```

This starts:
- Camera streaming
- Servo listener

---

### ğŸš€ 2. Start PC Tracking System

**Edit RPi IP:**
```python
RPI_IP = "your_rpi_ip_here"
```

**Run:**
```bash
python3 camera_servo_zmq.py
```

---

## ğŸ® Controls

| Key | Action |
|-----|--------|
| `c` | Recalibrate head center |
| `q` | Quit system |

---

## ğŸŒŸ What Was Planned (Eye Tracking)

The original idea was:
- Detect **pupil movement** â†’ turn servo
- Track **gaze direction** â†’ move camera

While this worked partially, gaze tracking was:
- âŒ Too noisy
- âŒ Required lighting calibration
- âŒ Hard to stabilize within the hackathon time

Thus the pivot to:
- âœ” Robust face landmark tracking
- âœ” Fast + stable
- âœ” Same user-intention (direction control)

---

## ğŸ“Œ Results

- âœ… Real-time head-controlled camera movement
- âœ… Smooth servo panning with <50ms latency
- âœ… Seamless Pi â†’ PC video streaming
- âœ… Robust performance even with partial occlusion

---

## ğŸ§­ Future Enhancements

- [ ] Switch back to eye-gaze tracking when time allows
- [ ] Add object detection + audio feedback
- [ ] Panâ€“tilt dual-servo rig
- [ ] Battery-powered wearable version
- [ ] Mobile app + Bluetooth control

---

## ğŸ‘¥ Team

Built at **DurHack 2025** for the Assistive Technology & Robotics track.

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgments

- **Mediapipe** for face landmark detection
- **Google** for Gemini apis
- **Raspberry Pi Foundation** for hardware support
- **DurHack 2025** organizers and mentors
